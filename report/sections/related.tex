%related work to your problem


A different part of the proposal by Whiteson explains that it is possible to use co-evolution to focus the agent on learning controllable rare events. The problem is that in order to use co-evolution to focus on controllable rare events, it is necessary to combat co-evolutionary forgetting, where previously acquired useful traits are lost in future evolutions. In this section, we will entail what research has been done so far that tries to accomplish this.

\cite{ficici2003game} propose to use a 'Nash Memory' mechanism, based on the game-theoretic principles of the Nash Equilibrium. This mechanism keeps track of important traits discovered during evolution and later uses this in forming a mixed policy. This guarantees monotonic progress, meaning that every change to the policy moves it closer to the true solution. Unfortunately, it does not guarantee finding policies of maximal value. In another paper by Ficici and Pollack, \cite{ficici2001pareto}, they propose to use pareto optimality to solve the problem of forgetting. In order to do this they treat the problem as a multi-objective optimization, which is unfortunately not possible in our setting.

Recently, a paper by Wilson et al. \cite{wilson2014using} was published that is similar to the research done in this project; using Gaussian Processes and Bayesian optimization to speed up the learning process at the expense of more computation cost. They achieve promising results, similar to the state of the art, with substantially less iterations necessary.

These recent works have made it plausible that combining co-evolution and GP-UCB techniques is a viable strategy. It also seems that this strategy has not been pursued before, which explains why there are only so few papers describing this and closely related techniques. 