In this section we will detail and motivate our approach. 

\subsection{Neural Network as Policy}

As discussed, we use a neural network as our policy. The inputs of such a network are the state and the outputs are the (encoded) action. Finding an optimal policy can then be defined as finding the optimal network topology and optimal weights for that topology.\\
This was also done by \cite{koppejan2011neuroevolutionary} in their approach to the helicopter control problem. Using a fixed, engineered network topology and optimizing weights with evolutionary policy search was shown to produce better results than jointly optimizing the weights and the topology. Furthermore, a network with a single layer of weights performed far worse than an MLP. Therefore, we focus on optimizing weights for an MLP with a fixed topology.\\
Specifically, %(activatiefuncie, aantal hidden, totaal aantal variabelen, blabla)

\subsection{Baseline: Evolutionary Policy Search}

In order to compare the performance of the GP-CEPS method, we used evolutionary programming to find good weights for the described Neural Network. 
The evolutionary algorithm we employed uses tournament selection to select organisms to breed with. The pipeline of the algorithm is as follows:\\ \\

\begin{algorithm}[ht]
  \caption{Genetic algorithm used for evolving organisms}
  \begin{algorithmic}
    \Input{$N \leftarrow N_{epochs}$}
    \Initialize{$pool \leftarrow randomPolicies(N_{pool})$}
    \For{t = 1 to N}
      \State $evaluate(pool)$
      \State $parents \leftarrow best(pool, N_{best})$
      \State $pool \leftarrow evolve(parents)$
    \EndFor
    \State $bestOrganism \leftarrow best(pool,1)$
  \end{algorithmic}
\end{algorithm}

We used $N_{pool} = 20$, $N_{best}$, $N_{epochs} = 100$. $evaluate(pool)$ was simply the rewards obtained in one episode simulation using the evaluated policy.  
A detailed description of the implementation of tournament selection and how the policies are mutated can be found in \cite{koppejan2011neuroevolutionary}. Further parameters used are Tournament Samplesize = 5, mutation probability = 0.75, mutation fraction = 0.2, mutation std = 1.0, and replacement probability = 0.2. 

\subsection{Gaussian Process Co-Evolutionary Policy Search}

The focus of our research is on developing a successful GP CEPS method. The main idea is to maintain a posterior distribution of the fitness $R_{\pi}^z$ over policies $\pi$ and outcomes $z$ with a Gaussian Process, where the outcome $z$ defines the nature of the stochastic transitions in an episode. \\
Tackling a problem in such a way introduces two optimization problems. The first is finding the best policy from the posterior distribution $R_{\pi}^z$. We will refer to this as the \textit{primary optimization problem}. The second problem is how to select $\pi$,$z$ combinations to evaluate with a simulation, in order to update $R_{\pi}^z$. We will refer to this as the \textit{secondary optimization problem}. \\
Both these problems can be solved with an evolutionary algorithm. This reduces the challenge for both problems to defining a good score function for within the evolutionary algorithm.

\subsubsection{Primary Optimization Problem}

As mentioned, defining an appropriate score function allows us to solve this problem with an evolutionary algorithm. In order to find the best policy from $R_{\pi}^z$, we need to marginalize over $z$ to obtain the expected rewards for policy $\pi$, $R_{\pi}$: 
\begin{align}
V_{\pi} = \int R(\pi | z) P(z)dz 
\end{align}
This can be done numerically by sampling $z$ from $P(z)$ and averaging the predicted $R_{\pi}^z$ for the scored $\pi$ and sampled $\mathbf{z}$. The rest of the evolutionary algorithm, including the parameters, is as described in Section 4.2.

\subsubsection{Secondary optimization problem: Acquisition Function}

Finding a good score measure for selecting $\pi$ and $z$ to simulate in order to update $R_{\pi}^z$ is much less trivial. As said, this can also be done with evolutionary programming, which reduces the problem to defining a good scoring measure. Of course, we want to obtain scores without actually simulating episodes in order to keep the sample complexity low, so in stead,  the information on $\pi$ and $z$ is extracted from the Gaussian Process. 

We want to evaluate \textit{promising} policies, i.e. policies with high predicted returns. On the other hand, we also want to include enough exploration, so we also want to evaluate policies with low predicted returns if their uncertainty is high. Therefore, in a deterministic setting (without $z$), this might lead to a UCB-like approach. %include reference?? 
However, in the current setting we want to select $\pi$, $z$ combinations. Furthermore, we want to select $z$ such that it is \textit{informative}: it must distinguish between policies. Taking these requirements into consideration, we defined the score for $\pi$ as: 

\begin{align}
Score_{\pi}(\pi ,\mathbf{Z}) = c_1 \sum_{i=1}^{N_{pool}} (R_{\pi}^{z_i} + c_2 \sigma_{\pi}^{z_i}(GP)) 
\end{align}
where $\sigma_{\pi}^{z_i}(GP)$ is the current standard deviation of the predicted mean of the GP, and $\mathbf{Z}$ is the current pool of $z$. 

The score for $z$ is defined as the variance over the expected returns for that $z$ and all $\pi$ in the pool:
\begin{align}
Score_z(z, \mathbf{\Pi}) = c_3 \cdot (\sum_{i=1}^{N_{pool}}(R_{\pi_i}^z-\sum_{i=1}^{N_{pool}}(R_{\pi_i}^z) )^2
\end{align}
Where $c_3$ is a weighting factor and $\mathbf{\Pi}$ is the current pool of policies.

\subsection{Toy Problem}

In order to somewhat simplify the problem, we design a Toy Problem on which we can test the GP-CEPS method. This way, we can start out with a smaller network, do not have long simulations, and can avoid long runtimes. Hereafter, we scale up the problem such that it is, comparable to the helicopter problem in the sense that the policy space has the same dimensionality. Specifically, we want to have the same amount of parameters to define the GP over. This way, the rewards as a function of the space spanned by policies and outcomes is comparable to that space in the Helicopter Control Problem.\\
After some ideas, improvements and iterations, the toy problem we ended up with is moving to a goal from a fixed starting position in a 2D plane. A one-time gust of wind with fixed direction, but varying strength is added when x-coordinate 0.5 is passed. The problem is further defined by:

\begin{itemize}
\item State: position in continuous x-y coordinates, each of which ranges between 0 and 1 while on the plane.
\item Actions: change in position in continuous x-y coordinates, with a maximum step size of 0.01 in both directions. 
\item This is an episodic task: the episode ends after the agent either reaches the goal state, or moves off the grid.
\item Start state and goal state: The start state is (0.1,0.1) and the goal state is anywhere in the circle around (0.85, 0.85) with radius 0.15.
\item Rewards: The reward function is defined as $R = 100 * \gamma^{i}$ if the agent reaches the goal where $i$ is the amount of steps taken. While the reward function is defined as $-R$ when the agent moves off the grid. We ran our experiments with $\gamma = 0.99$
\item State transition: $s_t = s_{t-1} + a_t + w$, where $w$ is the wind and sampled from a uniform distribution between 0 and 0.5
\end{itemize} 

\subsection{Contribution}
This project investigates the possibility of reducing sample complexity for policy search by applying co-evolution. Additionally, our approach, as proposed by Dr. White, handles co-evolutionary forgetting by viewing the problem in a bayesian setting. If succesfull, it would indicate that GP CEPS is a promising approach for problems where sample complexity is the bottleneck. Lastly, by evaluating policies on explicit outcomes, GP CEPS provides insights into the relevance of events, as the outcomes $z$ are selected for their rareness and controlability.

