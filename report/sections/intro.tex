%General introduction - motivation

\begin{itemize}
  \item{RL intro}
  \item{99\% cases okay, not rare events}
  \item{Co-evolution}
  \item{Maintain posterior believe of extended fitness}
\end{itemize}

In Reiforcement Learning an agent learns a policy by repeatedly acting in an environment and getting a reward for that action. In most Reinforcement Learning algorithms the agent learns a optimal policy by iteratively improving its current policy. However there is a class called normal direct policy-search methods where an agent finds its policy by directly searching the space of all possible policies. Because all policies have to be evaluated there is a lot of strain on the environment, where every action can put the agent in a bad state, these states might be costly, e.g. a robot-agent might break down because of its actions. To alleviate these problems in direct policy search methods we propose a new method. In our method we try to minimize the amount of policies that will be evaluated and still find an optimal policy. To do this we focus on controllable rare-events. These are events that occur infrequently where the agent could greatly influence the effect of the event. This means that some policies perform better than others in these events, making these events critical to distinguish between policies. In [49,50 van shimons proposal] it is shown that it is easy to find a policy that works $99\%$ of the time. But because the other $1\%$ of the time occurs so infrequently it is hard for an agent to learn to cope with these. It does this by maintaining a posterior believe of the extended fitness landscape. ....?