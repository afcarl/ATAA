% RL and Policy Search intro ( introducing helicopter too)
Reinforcement Learning is concerned with how agents behave in an environment such that the reward is maximized. The policy of an agent (its behavior) is learned by repeatedly acting in an environment and receiving a reward for each action given the state of the environment. One class of Reinforcement Learning is \textit{direct policy search}, a family of methods that directly searches the policy space in order to find the optimal policy. One of the areas where such policy search methods have proven useful is in control problems. However, further improvements upon existing methods are needed for efficiently discovering highly effective policies for complex tasks. An example of such a problem is the helicopter control problem \cite{abbeel2007application}, which is a problem where a simulation model exists to evaluate policies. Evolutionary algorithms have proven to perform reasonably well \cite{koppejan2011neuroevolutionary}, but unfortunately these algorithms have a high sample complexity. This becomes a problem when simulations are computationally costly. Another challenge in this problem is that the performance of a policy is determined largely by infrequent events, on which performance is rarely evaluated.

% Issue no controllable rare events are tested: solution = co-evolutionary approach
Focusing on controllable rare events (defined in more detail in section \ref{background}) for policy search would improve efficiency significantly. In order to be able to focus on particular events we define an outcome $z$, which specifies the result of every stochastic transition that could occur in a given episode. Thus for a given policy $\pi$ and outcome $z$, an episode is deterministic. Instead of wasting random trials on non-informative $z$, this project leverages co-evolutionary principles to evolve both policies $\pi$ as well as outcomes $z$. The main idea is to improve the evolution process of the population policies by evaluating them on a population of outcomes, which are evolved specially to test the policies.

% Issue evolutionary forgetting
A known issue in co-evolution is co-evolutionary forgetting \cite{ficici2003game}: that excel against new predictors may perform poorly against older ones. This comes from the fact that the new policies only get tested against these new outcomes, but not the older ones. By maintaining a posterior belief, in the form of a Gaussian Process (GP), over the fitness of policy-outcome pairs previous found results can be remembered. 

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.29]{images/fitness-landscape.png}
  \caption{Extended fitness landscape, where the reward is plotted given a policy $\pi$ and outcome $z$}\label{fitnesslandscape}
\end{figure}

% Approach in general
This posterior belief, as shown in figure \ref{fitnesslandscape}, attempts to map policy-outcome pairs to a reward. The challenge in this approach is to select appropriate pairs to evaluate and use as data point in the GP, called the acquisition function. Given a appropriate acquisition function, new data points are added to the landscape until a stop condition is met, afterwards the best policy is extracted from the Gaussian Process. We call this method \textbf{G}aussian \textbf{P}rocess - \textbf{C}o-\textbf{E}volutionary \textbf{P}olicy \textbf{S}earch, or \textbf{GP-CEPS} for short. \\


% outro: outline rest of paper
In order to describe our approach we first give some background of the techniques we use, followed by a description to what is already done to counter this problem. In section \ref{contrib} our approach is discussed in detail followed by the experiments we ran and their results. Finally, we will discuss our experiments and results and draw conclusions.


