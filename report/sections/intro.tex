% ver 1 Still used for inspiration!
%In Reiforcement Learning an agent learns a policy by repeatedly acting in an environment and getting a reward for that action. In most Reinforcement Learning algorithms the agent learns a optimal policy by iteratively improving its current policy. However there is a class called normal direct policy-search methods where an agent finds its policy by directly searching the space of all possible policies. Because all policies have to be evaluated there is a lot of strain on the environment, where every action can put the agent in a bad state, these states might be costly, e.g. a robot-agent might break down because of its actions. To alleviate these problems in direct policy search methods we propose a new method. In our method we try to minimize the amount of policies that will be evaluated and still find an optimal policy. To do this we focus on controllable rare-events. These are events that occur infrequently where the agent could greatly influence the effect of the event. This means that some policies perform better than others in these events, making these events critical to distinguish between policies. In \cite{koppejan2011neuroevolutionary} it is shown that it is easy to find a policy that works $99\%$ of the time. But because the other $1\%$ of the time occurs so infrequently it is hard for an agent to learn to cope with these. 
%We use co-evolution[ref coevo] to train a pool of agents and a pool of organisms that govern the stochastic nature of the environment, called outcomes. With this we hope to train the agents in such a way that they will learn to behave optimally against all outcome-organisms. However there is'co-evolutionary forgetting'[REF CO-Forget]. Where an agent has learned a certain behaviour that performs well against the other pool, but as this pool changes it forgets this behaviour as it is not as good against the new organisms, when it encounters new organisms against which this behaviour was good it will have forgotten how it should act. In our case this can be seen as the situation where an agent forgets how to behave optimally against outcome-organisms that were present in the past. To fight this problem we maintain a posterior believe of the extended fitness landscape, how a policy works against a outcome-organism. We maintain this believe by using a Gaussian Process.

% end ver 1 used for inspiration


% RL intro ( introducing helicopter too)
Reinforcement learning is concerned with how agents should take actions in an environment such that the reward is maximized. The behavior of the agent (it's policy) is learned by repeatedly acting in an environment and receiving a reward for each action given the state of the environment. This can be done either by directly improving the current policy, as by directly searching for an optimal policy in the policy space. As interaction with the environment can potentially have high costs (such as an agent breaking down), a fundamental issue is the evaluation of a policy (as this often requires multiple trials). A key insight is that many trials are wasted because they do not focus on interesting events to evaluate policies on. 

% TODO: in [ref helictor] for example, such and such results were produced, but in rare event this and that a large room for improvement was present.

% Issue no controllable rare events are tested: solution = co-evolutionary approach
Focusing on controllable rare events (defined in more detail in section \ref{background}) for policy search would improve efficiency significantly. In order to be able to focus on particular events we define an outcome $z$, which specifies the result of every stochastic transition that could occur in a given episode. Given a policy $\pi$ and outcome $z$, an episode is thus deterministic. Instead of wasting random trials (using non-informative $z$), this project leverages co-evolutionary principles to evolve both policies $\pi$ as outcomes $z$. The main idea is to improve the evolution process of the population policies by evaluating them on a population of outcomes, which are evolved specially to test the policies.

% Issue evolutionary forgetting
%Known issues.....

% Approach in general
% maybe not necessary

% outro: outline rest of paper
% In section....


