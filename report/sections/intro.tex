In Reiforcement Learning an agent learns a policy by repeatedly acting in an environment and getting a reward for that action. In most Reinforcement Learning algorithms the agent learns a optimal policy by iteratively improving its current policy. However there is a class called normal direct policy-search methods where an agent finds its policy by directly searching the space of all possible policies. Because all policies have to be evaluated there is a lot of strain on the environment, where every action can put the agent in a bad state, these states might be costly, e.g. a robot-agent might break down because of its actions. To alleviate these problems in direct policy search methods we propose a new method. In our method we try to minimize the amount of policies that will be evaluated and still find an optimal policy. To do this we focus on controllable rare-events. These are events that occur infrequently where the agent could greatly influence the effect of the event. This means that some policies perform better than others in these events, making these events critical to distinguish between policies. In [49,50 van shimons proposal] it is shown that it is easy to find a policy that works $99\%$ of the time. But because the other $1\%$ of the time occurs so infrequently it is hard for an agent to learn to cope with these. 
We use co-evolution[ref coevo] to train a pool of agents and a pool of organisms that govern the stochastic nature of the environment, called outcomes. With this we hope to train the agents in such a way that they will learn to behave optimally against all outcome-organisms. However there is something called 'co-evolutionary forgetting'[REF CO-Forget], in our case this can be seen as the situation where an agent forgets how to behave optimally against outcome-organisms that were present in earlier stages of the evolutionary algorithm. To fight this problem we maintain a posterior believe of the extended fitness landscape, how a policy works against a outcome-organism. We maintain this believe by using a Gaussian Process.